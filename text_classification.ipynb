{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchtext\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Check the settings\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "train_dataset = load_dataset(\n",
    "    \"ucirvine/reuters21578\", \"ModApte\", split=\"train\", trust_remote_code=True\n",
    ").with_format(\"torch\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"ucirvine/reuters21578\", \"ModApte\", split=\"test\", trust_remote_code=True\n",
    ").with_format(\"torch\")\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\n",
    "    [col for col in train_dataset.column_names if col not in [\"text\", \"topics\"]]\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.remove_columns(\n",
    "    [col for col in test_dataset.column_names if col not in [\"text\", \"topics\"]]\n",
    ")\n",
    "\n",
    "unique_topics = set()\n",
    "\n",
    "for entry in train_dataset:\n",
    "    unique_topics.update(entry[\"topics\"])\n",
    "\n",
    "for entry in test_dataset:\n",
    "    unique_topics.update(entry[\"topics\"])\n",
    "\n",
    "print(f\"Number of unique topics: {len(unique_topics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[\"text\"])\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_dataset),\n",
    "    specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
    "    min_freq=2,\n",
    "    special_first=True,\n",
    ")\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = T.Sequential(\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    T.AddToken(1, begin=True),\n",
    "    T.Truncate(512),\n",
    "    T.AddToken(2, begin=False),\n",
    "    T.ToTensor(),\n",
    ")\n",
    "\n",
    "print(\n",
    "    pad_sequence(\n",
    "        [text_transform(tokenizer(\"Hello world\"))],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab[\"<pad>\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "text_tokenizer = lambda batch: [tokenizer(x) for x in batch]\n",
    "topic_to_idx = {topic: idx for idx, topic in enumerate(unique_topics)}\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for data_sample in batch:\n",
    "        if not data_sample[\"topics\"]:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            processed_text = text_transform(tokenizer(data_sample[\"text\"]))\n",
    "            text_list.append(processed_text)\n",
    "            label_list.append(topic_to_idx[data_sample[\"topics\"][0]])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data sample: {data_sample}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    return text_list, label_list\n",
    "\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "data_loader_test = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.5,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_input, mem_input):\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        output, (hidden_output, mem_output) = self.lstm(\n",
    "            input_embs, (hidden_input, mem_input)\n",
    "        )\n",
    "        return self.fc(output), hidden_output, mem_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "num_layers = 3\n",
    "hidden_size = 128\n",
    "num_emb = len(vocab)\n",
    "out_size = len(unique_topics)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = LSTM(\n",
    "    num_emb=num_emb,\n",
    "    output_size=out_size,\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "training_acc_logger = []\n",
    "test_acc_logger = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(data_loader_train):\n",
    "        bs = target.shape[0]\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "\n",
    "        pred, hidden, memory = model(data, hidden, memory)\n",
    "        loss = loss_fn(pred[:, -1, :], target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred[:, -1, :], 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(data_loader_train):.4f}, Training Accuracy: {train_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader_test):\n",
    "            bs = target.shape[0]\n",
    "            hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "            memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "\n",
    "            pred, hidden, memory = model(data, hidden, memory)\n",
    "            loss = loss_fn(pred[:, -1, :], target)\n",
    "\n",
    "            _, predicted = torch.max(pred[:, -1, :], 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            test_loss += loss.item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss / len(data_loader_test):.4f}, Test Accuracy: {test_acc:.2f}%\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
